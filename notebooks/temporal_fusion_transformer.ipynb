{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "\n",
    "# load data\n",
    "data = ...\n",
    "\n",
    "# define dataset\n",
    "max_encode_length = 36\n",
    "max_prediction_length = 6\n",
    "training_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.date <= training_cutoff],\n",
    "    time_idx= ...,\n",
    "    target= ...,\n",
    "    group_ids=[ ... ],\n",
    "    max_encode_length=max_encode_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[ ... ],\n",
    "    static_reals=[ ... ],\n",
    "    time_varying_known_categoricals=[ ... ],\n",
    "    time_varying_known_reals=[ ... ],\n",
    "    time_varying_unknown_categoricals=[ ... ],\n",
    "    time_varying_unknown_reals=[ ... ],\n",
    ")\n",
    "\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=0,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=2,\n",
    "    reduce_on_plateau_patience=4\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.lr_find(\n",
    "    tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "\n",
    "trainer.fit(\n",
    "    tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
